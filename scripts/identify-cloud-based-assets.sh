#!/bin/bash
#############################################################################################
# Script to find, as much as possible, Cloud based services (AWS/AZURE/GCP) 
# used by a company based on its main web domain, as input information.
#
# Based on the following project for AZURE information:
# 	https://github.com/NetSPI/MicroBurst/blob/master/Misc/Invoke-EnumerateAzureSubDomains.ps1
#
# Requirements in terms of software:
#	https://github.com/iknowjason/edge
# 	https://github.com/projectdiscovery/httpx
#	https://github.com/d3mondev/puredns
#	https://github.com/ffuf/ffuf
# 	apt install wget curl jq dnsutils whois nmap
# 
# Note:
#	System command was used, for DNS related work, because it was more reliable than 
#	"github.com/projectdiscovery/dnsx" when the container was used within the 
#	context of an VPN connection on the docker host.
#
# Remark:
#	Focus was made on reliability of data retrieved over the performance aspects.
#############################################################################################

# Constants
INIT_MARKER_FILE="/tmp/$(basename $0).init"
PERMUTATIONS_FILE="/tmp/permutations.txt"
CRTSH_DATA_FILE="/tmp/crtshdata.json"
WORK_FILE="/tmp/work.txt"
CSV_FILE="assets.csv"
CSV_FILE_HEADERS="Provider,Subdomain,IPv4"
THREAD_COUNT=10
DNS_RATE_LIMIT_COUNT=500
DNS_WORDLIST="/tools/sec-lists/Discovery/DNS/namelist.txt"
GO_BIN_HOME="/root/go/bin"
EDGE_HOME="/tools/edge"
EDGE_TRACE_FILE="edge_trace.txt"
AZURE_SUBDOMAINS=("onmicrosoft.com" "scm.azurewebsites.net" "azurewebsites.net" "p.azurewebsites.net" "cloudapp.net" "file.core.windows.net" "blob.core.windows.net" "queue.core.windows.net" "table.core.windows.net" "mail.protection.outlook.com" "sharepoint.com" "redis.cache.windows.net" "documents.azure.com" "database.windows.net" "vault.azure.net" "azureedge.net" "search.windows.net" "azure-api.net" "azurecr.io")
EXTRA_PROVIDERS_SUBDOMAINS=("service-now.com")
PROVIDERS_SUBDOMAINS=("${AZURE_SUBDOMAINS[@]}" "${EXTRA_PROVIDERS_SUBDOMAINS[@]}")

# Entry point
if [ "$#" -lt 1 ]; then
    script_name=$(basename "$0")
    echo "Usage:"
    echo "   $script_name [COMPANY_BASE_DOMAIN]"
    echo ""
    echo "Call example:"
    echo "    $script_name righettod.eu"
    exit 1
fi

# Utility functions
function write_step(){
    echo -e "\e[93m$1\e[0m"
}

function extract_dns_san_from_tls_cert(){
	subdomain=$1
	#san_entries=$(openssl s_client -connect $subdomain:443 </dev/null 2>/dev/null | openssl x509 -noout -text | grep -E "DNS:(.*)" | sed 's/DNS://g' | tr -d ' ')
	san_entries=$(nmap -Pn -p 443 --script ssl-cert $subdomain 2>/dev/null | grep -F "Subject Alternative Name:" | sed 's/Subject Alternative Name://g' | cut -d'|' -f2 | tr -d " " | sed 's/DNS://g')
	if [ ${#san_entries} -gt 1 ]
	then
		for san_entry in $(echo $san_entries | tr "," "\n")
		do
			is_star_san=$(echo $san_entry | grep -Fc "*")
			if [ "is_star_san" == "0" ]
			then
				process_subdomain $san_entry
			fi
		done
	fi
	
}

function process_subdomain(){
	subdomain=$1
	for ipv4 in $(dig +time=2 +tries=2 +retry=1 +short A $subdomain)
	do
		is_ipv4=$(echo "$ipv4" | grep -Ec "^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}$")
		if [ $is_ipv4 -ne 1 ]
		then
			continue
		fi
		whois_record=$(whois $ipv4 2>/dev/null)
		is_azure=$(echo "$whois_record" | grep -iFc "MICROSOFT")
		is_aws=$(echo "$whois_record" | grep -iFc "AMAZON")
		is_gcp=$(echo "$whois_record" | grep -iFc "GOOGLE")
		provider="NONE"
		if [ $is_azure -gt 0 ]
		then
			provider="AZURE"
		elif [ $is_aws -gt 0 ]
		then
			provider="AWS"
		elif [ $is_gcp -gt 0 ]
		then
			provider="GCP"			
		fi
		if [ "$provider" != "NONE" ]
		then
			print_result_line "$provider" "$subdomain" "$ipv4"
		fi		
	done	
}

function print_result_line(){
	provider=$1
	subdomain=$2
	ipv4=$3	
	printf "[%-5s] %-15s => %s\n" "$provider" "$ipv4" "$subdomain"
	echo "$provider,$subdomain,$ipv4" >> $CSV_FILE
}

function validate_downloaded_data_files(){
	status=0
	counter=$(grep -Fc "$BASE_DOMAIN" $CRTSH_DATA_FILE)
	if [ $counter -eq 0 ]
	then
		status=1
	fi
	if [ ! -s $PERMUTATIONS_FILE ]
	then
		status=2
	fi
	if [ ! -s $EDGE_HOME/ip-ranges.json ]
	then
		status=3
	fi		
	if [ ! -s $EDGE_HOME/goog.json ]
	then
		status=4
	fi			
	if [ ! -s $EDGE_HOME/azure.json ]
	then
		status=5
	fi
	return $status
}

# Working context
BASE_DOMAIN="$1"
BASE=$(echo $BASE_DOMAIN | cut -d'.' -f1)

# Main processing
rm $CSV_FILE 2>/dev/null
write_step "[Discovery] Download cloud providers and sub domains data for domain '$BASE_DOMAIN' using base as '$BASE'..."
curl -sk --output $CRTSH_DATA_FILE "https://crt.sh/?q=%25.$BASE_DOMAIN&output=json"
if [ ! -f $INIT_MARKER_FILE ]
then 
	curl -sk https://raw.githubusercontent.com/NetSPI/MicroBurst/master/Misc/permutations.txt | sed 's/\$//g' > $PERMUTATIONS_FILE
	curl -sk https://azureipranges.azurewebsites.net/Home/Update > /dev/null
	wget -q -O $EDGE_HOME/ip-ranges.json https://ip-ranges.amazonaws.com/ip-ranges.json
	wget -q -O $EDGE_HOME/goog.json https://www.gstatic.com/ipranges/goog.json
	wget -q -O $EDGE_HOME/azure.json https://azureipranges.azurewebsites.net/Data/Public.json
	validate_downloaded_data_files
	validity_check=$?
	if [ $validity_check -ne 0 ]
	then
		echo "Data files were not correctly downloaded, please retry (status code is $validity_check)."
		rm $INIT_MARKER_FILE 2>/dev/null
		exit 1		
	fi
	echo "X" > $INIT_MARKER_FILE
	echo "Done."
else
	echo "Already performed."
fi
ls -l $PERMUTATIONS_FILE $CRTSH_DATA_FILE $EDGE_HOME/*.json
write_step "[Discovery] AWS Federated WorkSpaces via HTTP discovery..."
# Direct case
echo $BASE > $WORK_FILE
while IFS= read -r word
do
	# Prefix case
	subdomain="$word$BASE"
	echo $subdomain >> $WORK_FILE
	# Suffix case	
	subdomain="$BASE$word"
	echo $subdomain >> $WORK_FILE		
done < "$PERMUTATIONS_FILE"
ffuf -s -w $WORK_FILE -u "https://FUZZ.awsapps.com/zocalo" -mc 200 -t $THREAD_COUNT -of json -o $WORK_FILE.tmp 1>/dev/null
mv $WORK_FILE.tmp $WORK_FILE
for line in $(cat $WORK_FILE | jq -r ".results[].host")
do
	process_subdomain $line
done
write_step "[Discovery] AZURE services via DNS discovery..."
for sub_domain in "${PROVIDERS_SUBDOMAINS[@]}"
do
	# Direct case
	subdomain="$BASE.$sub_domain"
	process_subdomain $subdomain
	while IFS= read -r word
	do
		# Prefix case
		subdomain="$word$BASE.$sub_domain"
		process_subdomain $subdomain
		# Suffix case	
		subdomain="$BASE$word.$sub_domain"
		process_subdomain $subdomain		
	done < "$PERMUTATIONS_FILE"
done
write_step "[Discovery] AZURE/AWS/GCP services via DNS discovery..."
rm $WORK_FILE 2>/dev/null
$GO_BIN_HOME/puredns bruteforce "$DNS_WORDLIST" "$BASE_DOMAIN" --write $WORK_FILE  --rate-limit-trusted $DNS_RATE_LIMIT_COUNT --rate-limit $DNS_RATE_LIMIT_COUNT --quiet >/dev/null
while IFS= read -r subdomain
do
	process_subdomain $subdomain
done < "$WORK_FILE"
write_step "[Discovery] AWS S3 buckets via HTTP discovery..."
aws_s3_domain="s3.amazonaws.com"
# Direct case
subdomain="$BASE.$aws_s3_domain"
echo $subdomain > $WORK_FILE
while IFS= read -r word
do
	# Prefix case
	subdomain="$word$BASE.$aws_s3_domain"
	echo $subdomain >> $WORK_FILE
	# Suffix case	
	subdomain="$BASE$word.$aws_s3_domain"
	echo $subdomain >> $WORK_FILE		
done < "$PERMUTATIONS_FILE"
$GO_BIN_HOME/httpx  -duc -silent -fe "(NoSuchBucket|IllegalLocationConstraintException)" -title -t $THREAD_COUNT -rl $THREAD_COUNT -list $WORK_FILE -json > $WORK_FILE.tmp
mv $WORK_FILE.tmp $WORK_FILE
for line in $(cat $WORK_FILE)
do
	print_result_line "AWS" "$(echo $line | jq -r '.input')" "$(echo $line | jq -r '.host')"
done
write_step "[Discovery] AZURE/AWS/GCP services via 'crt.sh' entries (CN entries)..."
cat $CRTSH_DATA_FILE | jq -r ".[].common_name" | sort -u | grep -iv "\.local$" | grep -Fv "*" > $WORK_FILE
while IFS= read -r subdomain
do
	process_subdomain $subdomain
done < "$WORK_FILE"
write_step "[Discovery] AZURE/AWS/GCP services via 'crt.sh' entries (SAN entries)..."
cat $CRTSH_DATA_FILE | jq -r ".[].name_value" | sort -u | grep -iv "\.local$" | grep -Fv "*" | grep -Fv "@" > $WORK_FILE
while IFS= read -r subdomain
do
	process_subdomain $subdomain
done < "$WORK_FILE"
write_step "[Discovery] AZURE/AWS/GCP services via the 'EDGE' tools..."
echo "[i] See file '$EDGE_TRACE_FILE' for Edge raw results in case of later need."
cdir=$(pwd)
cd $EDGE_HOME
./edge -domain "$BASE_DOMAIN" -dns -crt -silent > $WORK_FILE
cd $cdir
cp $WORK_FILE $EDGE_TRACE_FILE
cat $WORK_FILE | grep -F ",A," | cut -d',' -f1 | sort -u > $WORK_FILE.tmp
mv $WORK_FILE.tmp $WORK_FILE
while IFS= read -r subdomain
do
	process_subdomain $subdomain
done < "$WORK_FILE"
write_step "[Gathering] Assemble the final CSV file '$CSV_FILE'..."
echo $CSV_FILE_HEADERS > $WORK_FILE
cat $CSV_FILE | sort -u >> $WORK_FILE
mv $WORK_FILE $CSV_FILE
rm $CRTSH_DATA_FILE
